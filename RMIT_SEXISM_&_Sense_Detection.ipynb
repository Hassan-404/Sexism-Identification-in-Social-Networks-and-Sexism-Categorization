{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h-j2WGhAZ_C2",
        "7q1obyYzaInK",
        "TfL9qkvZDeMM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Upload training file and necessary preprocessing"
      ],
      "metadata": {
        "id": "RKSwPQ63ZaYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQqjpvdBYMGE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Requisits"
      ],
      "metadata": {
        "id": "orNCu9lLp5Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langdetect\n",
        "!pip install numpy==1.23.5\n",
        "!pip install -q -U \"tensorflow-text==2.11.*\"\n",
        "!pip install -q tf-models-official==2.11.0"
      ],
      "metadata": {
        "id": "D5DxubJSp4Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langdetect import detect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "training_set = pd.read_json(\"EXIST2023_training.json\")\n",
        "dev_set = pd.read_json(\"EXIST2023_dev.json\")\n",
        "test_set = pd.read_json(\"EXIST2023_test_clean.json\")"
      ],
      "metadata": {
        "id": "JynGyd6ZYV2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the dataset "
      ],
      "metadata": {
        "id": "UbOBvY7pZtAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_d = []\n",
        "labels_d = []\n",
        "sexism_d = []\n",
        "tLabels_d = []\n",
        "sLabels_d = []\n",
        "otherLang_d = 0\n",
        "english_d = 0\n",
        "id_d = []\n",
        "sexId_d = []\n",
        "\n",
        "for i in dev_set:  \n",
        "  countY = 0\n",
        "  countN = 0\n",
        "\n",
        "  # if detect(df[i].tweet) == 'en':\n",
        "  #   english +=1\n",
        "  # else:\n",
        "  #   otherLang +=1\n",
        "  #   continue\n",
        "\n",
        "  if (i < 200000) or (i >= 203260 and i <=400001) :\n",
        "    continue\n",
        "  id_d.append(int(i))\n",
        "  # print(detect(df[i].tweet))\n",
        "  tweets_d.append(dev_set[i].tweet)\n",
        "\n",
        "  for l in dev_set[i].labels_task1:\n",
        "    if l == 'YES':\n",
        "      countY +=1\n",
        "    if l == 'NO':\n",
        "      countN +=1\n",
        "  if countY == countN :\n",
        "    labels_d.append(-1)\n",
        "  elif countY > countN:\n",
        "    labels_d.append(1)\n",
        "    sexId_d.append(i)\n",
        "    sexism_d.append(dev_set[i].tweet)\n",
        "    tune = {\n",
        "        \"REPORTED\": 0, \"JUDGEMENTAL\": 0, \"DIRECT\": 0\n",
        "    }\n",
        "    sense = {\n",
        "        \"MISOGYNY-NON-SEXUAL-VIOLENCE\":0,\n",
        "        \"IDEOLOGICAL-INEQUALITY\": 0, \n",
        "        \"STEREOTYPING-DOMINANCE\": 0, \n",
        "        \"OBJECTIFICATION\": 0, \n",
        "        \"SEXUAL-VIOLENCE\":0, \n",
        "        \"-1\":0\n",
        "    }\n",
        "    for m in dev_set[i].labels_task2:\n",
        "      if m == 'REPORTED':\n",
        "        tune[\"REPORTED\"] +=1\n",
        "      if m == 'JUDGEMENTAL':\n",
        "        tune[\"JUDGEMENTAL\"] +=1\n",
        "      if m == 'DIRECT':\n",
        "        tune[\"DIRECT\"] +=1 \n",
        "    tLabels_d.append(max(tune, key=tune.get))\n",
        "\n",
        "    for n in dev_set[i].labels_task3:\n",
        "\n",
        "      if n[0] == 'MISOGYNY-NON-SEXUAL-VIOLENCE':\n",
        "        sense[\"MISOGYNY-NON-SEXUAL-VIOLENCE\"] +=1\n",
        "      if n[0] == 'IDEOLOGICAL-INEQUALITY':\n",
        "        sense[\"IDEOLOGICAL-INEQUALITY\"] +=1\n",
        "      if n[0] == 'STEREOTYPING-DOMINANCE':\n",
        "        sense[\"STEREOTYPING-DOMINANCE\"] +=1\n",
        "      if n[0] == 'OBJECTIFICATION':\n",
        "        sense[\"OBJECTIFICATION\"] +=1\n",
        "      if n[0] == 'SEXUAL-VIOLENCE':\n",
        "        sense[\"SEXUAL-VIOLENCE\"] +=1\n",
        "      if n[0] == '-':\n",
        "        sense[\"-1\"] +=1\n",
        "\n",
        "    sLabels_d.append(max(sense, key=sense.get))\n",
        "  else:\n",
        "    labels_d.append(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "7VgvHp1ucyKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "labels = []\n",
        "sexism = []\n",
        "tLabels = []\n",
        "sLabels = []\n",
        "otherLang = 0\n",
        "english = 0\n",
        "id = []\n",
        "sexId = []\n",
        "\n",
        "for i in training_set :  \n",
        "  countY = 0\n",
        "  countN = 0\n",
        "\n",
        "  # if detect(df[i].tweet) == 'en':\n",
        "  #   english +=1\n",
        "  # else:\n",
        "  #   otherLang +=1\n",
        "  #   continue\n",
        "\n",
        "  if (i < 200000) or (i >= 203260 and i <=400001) :\n",
        "    continue\n",
        "  id.append(int(i))\n",
        "  # print(detect(df[i].tweet))\n",
        "  tweets.append(training_set[i].tweet)\n",
        "\n",
        "\n",
        "  for l in training_set[i].labels_task1:\n",
        "    if l == 'YES':\n",
        "      countY +=1\n",
        "    if l == 'NO':\n",
        "      countN +=1\n",
        "  if countY == countN :\n",
        "    labels.append(-1)\n",
        "  elif countY > countN:\n",
        "    labels.append(1)\n",
        "    sexId.append(i)\n",
        "    sexism.append(training_set[i].tweet)\n",
        "    tune = {\n",
        "        \"REPORTED\": 0, \"JUDGEMENTAL\": 0, \"DIRECT\": 0\n",
        "    }\n",
        "    sense = {\n",
        "        \"MISOGYNY-NON-SEXUAL-VIOLENCE\":0,\n",
        "        \"IDEOLOGICAL-INEQUALITY\": 0, \n",
        "        \"STEREOTYPING-DOMINANCE\": 0, \n",
        "        \"OBJECTIFICATION\": 0, \n",
        "        \"SEXUAL-VIOLENCE\":0, \n",
        "        \"-1\":0\n",
        "    }\n",
        "    for m in training_set[i].labels_task2:\n",
        "      if m == 'REPORTED':\n",
        "        tune[\"REPORTED\"] +=1\n",
        "      if m == 'JUDGEMENTAL':\n",
        "        tune[\"JUDGEMENTAL\"] +=1\n",
        "      if m == 'DIRECT':\n",
        "        tune[\"DIRECT\"] +=1 \n",
        "    tLabels.append(max(tune, key=tune.get))\n",
        "\n",
        "    for n in training_set[i].labels_task3:\n",
        "\n",
        "      if n[0] == 'MISOGYNY-NON-SEXUAL-VIOLENCE':\n",
        "        sense[\"MISOGYNY-NON-SEXUAL-VIOLENCE\"] +=1\n",
        "      if n[0] == 'IDEOLOGICAL-INEQUALITY':\n",
        "        sense[\"IDEOLOGICAL-INEQUALITY\"] +=1\n",
        "      if n[0] == 'STEREOTYPING-DOMINANCE':\n",
        "        sense[\"STEREOTYPING-DOMINANCE\"] +=1\n",
        "      if n[0] == 'OBJECTIFICATION':\n",
        "        sense[\"OBJECTIFICATION\"] +=1\n",
        "      if n[0] == 'SEXUAL-VIOLENCE':\n",
        "        sense[\"SEXUAL-VIOLENCE\"] +=1\n",
        "      if n[0] == '-':\n",
        "        sense[\"-1\"] +=1\n",
        "\n",
        "    sLabels.append(max(sense, key=sense.get))\n",
        "  else:\n",
        "    labels.append(0)\n",
        "\n",
        "\n",
        "# print(\"English tweets \", english)\n",
        "# print(\"Other tweets \", otherLang)"
      ],
      "metadata": {
        "id": "HErY8AC5YZ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = []\n",
        "tweet_id_t = []\n",
        "for i in test_set:\n",
        "  if i < 600000:\n",
        "    continue\n",
        "  tweet_id_t.append(i)\n",
        "  test_tweets.append(test_set[i].tweet)"
      ],
      "metadata": {
        "id": "YAXNblGsdhKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfW2k6dMeb0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put the dataset into dataframe"
      ],
      "metadata": {
        "id": "a-k_GU8XZ6mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets = pd.DataFrame(\n",
        "    {'id': id,\n",
        "     'tweets': tweets,\n",
        "    })\n",
        "\n",
        "df_sexism = pd.DataFrame(\n",
        "    {'id': sexId,\n",
        "     'sexism': sexism,\n",
        "    })\n",
        "\n",
        "dev_tweets = pd.DataFrame( \n",
        "    {\n",
        "        'id': id_d,\n",
        "     'tweets': tweets_d,\n",
        "    })\n",
        "\n",
        "dev_sexism = pd.DataFrame(\n",
        "    {'id': sexId_d,\n",
        "     'sexism': sexism_d,\n",
        "    })\n",
        "\n",
        "test_set = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": tweet_id_t,\n",
        "        \"tweet\": test_tweets\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "mrF3iIzYYicM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preprocessing the Tweets"
      ],
      "metadata": {
        "id": "TeTvs_9JaCF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "## 1. Removal of punctuation and capitlization\n",
        "## 2. Tokenizing\n",
        "## 3. Removal of stopwords\n",
        "## 4. Stemming\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
        "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
        "stopwords.extend(other_exclusions)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_cleansing(tweet):  \n",
        "    \n",
        "    # removal of extra spaces\n",
        "    re_pattern = re.compile(r'\\s+')\n",
        "    spaceRefined = tweet.str.replace(re_pattern, ' ')\n",
        "\n",
        "    # removal of @name[mention]\n",
        "    re_pattern = re.compile(r'@[\\w\\-]+')\n",
        "    usernameRefined = spaceRefined.str.replace(re_pattern, '')\n",
        "\n",
        "    # removal of links[https://abc.com]\n",
        "    re_pattern =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    tweets = usernameRefined.str.replace(re_pattern, '')\n",
        "    \n",
        "    # removal of punctuations and numbers\n",
        "    puncRefined = tweets.str.replace(\"[^a-zA-Z]\", \" \")\n",
        "    # remove whitespace with a single space\n",
        "    newtweet = puncRefined.str.replace(r'\\s+', ' ')\n",
        "    # remove leading and trailing whitespace\n",
        "    newtweet = newtweet.str.replace(r'^\\s+|\\s+?$','')\n",
        "    # replace normal numbers with numbr\n",
        "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
        "    # removal of capitalization\n",
        "    tweet_lower = newtweet.str.lower()\n",
        "    \n",
        "    # tokenizing\n",
        "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
        "    \n",
        "    # removal of stopwords\n",
        "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
        "    \n",
        "    # stemming of the tweets\n",
        "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
        "    \n",
        "    for i in range(len(tokenized_tweet)):\n",
        "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "        tweets_p= tokenized_tweet\n",
        "    \n",
        "    return tweets_p"
      ],
      "metadata": {
        "id": "nOvxwVTYYqp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refinedTweets = data_cleansing(df_tweets.tweets)   \n",
        "df_tweets['processed_tweets'] = refinedTweets\n",
        "print(df_tweets[[\"tweets\",\"processed_tweets\"]].head(10))"
      ],
      "metadata": {
        "id": "2WyKHOmvYwlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refinedTweets = data_cleansing(dev_tweets.tweets)   \n",
        "dev_tweets['processed_tweets'] = refinedTweets\n",
        "print(dev_tweets[[\"tweets\",\"processed_tweets\"]].head(10))"
      ],
      "metadata": {
        "id": "HQhkrMfaeKN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refinedTweets = data_cleansing(df_sexism.sexism)   \n",
        "df_sexism['processed_tweets'] = refinedTweets\n",
        "print(df_sexism[[\"sexism\",\"processed_tweets\"]].head(10))"
      ],
      "metadata": {
        "id": "BtiZDye23Of2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refinedTweets = data_cleansing(dev_sexism.sexism)   \n",
        "dev_sexism['processed_tweets'] = refinedTweets\n",
        "print(dev_sexism[[\"sexism\",\"processed_tweets\"]].head(10))"
      ],
      "metadata": {
        "id": "EBhFxjjWeQD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refinedTweets = data_cleansing(test_set.tweet)   \n",
        "test_set['processed_tweets'] = refinedTweets\n",
        "print(test_set[[\"tweet\",\"processed_tweets\"]].head(10))"
      ],
      "metadata": {
        "id": "5RjlXP4Vfz08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting the labels into dataframe"
      ],
      "metadata": {
        "id": "rXTVpKE9aQuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sexism['label'] = sLabels\n",
        "df_tweets['label'] = labels\n",
        "dev_sexism['label'] = sLabels_d\n",
        "dev_tweets['label'] = labels_d\n"
      ],
      "metadata": {
        "id": "LdRUXOJBYx8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of all tweets in the dataset and Sexism Tweets"
      ],
      "metadata": {
        "id": "rDan8K8maYBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing which of the word is most commonly used in the dataset\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "all_words = ' '.join([word for word in df_tweets['processed_tweets'] ])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "#random=0.30\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C-DKj6qaY3ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing which of the word is most commonly used in the dataset\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "all_words = ' '.join([word for word in df_sexism['processed_tweets'] ])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "#random=0.30\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDhUDrlk-k4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing dataset labels for Machine Learning Algorithms"
      ],
      "metadata": {
        "id": "ZCbtDJ8HahNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Labelling"
      ],
      "metadata": {
        "id": "XVf3K5i9vsc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets = df_tweets[df_tweets['label'] != -1]\n",
        "df_tweets['label'].value_counts()"
      ],
      "metadata": {
        "id": "COzEM1dvY6VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_tweets = dev_tweets[dev_tweets['label'] != -1]\n",
        "dev_tweets['label'].value_counts()"
      ],
      "metadata": {
        "id": "6CNKaCU1e7Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sexism = df_sexism[df_sexism['label'] != \"-1\"]\n",
        "df_sexism['label'].value_counts()"
      ],
      "metadata": {
        "id": "BuIAso5-Sqvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_sexism = dev_sexism[dev_sexism['label'] != \"-1\"]\n",
        "dev_sexism['label'].value_counts()"
      ],
      "metadata": {
        "id": "krM_lw4yfCvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sexism['label']=df_sexism['label'].replace('IDEOLOGICAL-INEQUALITY', 0)\n",
        "df_sexism['label']=df_sexism['label'].replace('STEREOTYPING-DOMINANCE', 1)\n",
        "df_sexism['label']=df_sexism['label'].replace('OBJECTIFICATION', 2)\n",
        "df_sexism['label']=df_sexism['label'].replace('MISOGYNY-NON-SEXUAL-VIOLENCE', 3)\n",
        "df_sexism['label']=df_sexism['label'].replace('SEXUAL-VIOLENCE', 4)\n",
        "df_sexism['label'].value_counts()"
      ],
      "metadata": {
        "id": "aTxW7SEO3ajD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_sexism['label']=dev_sexism['label'].replace('IDEOLOGICAL-INEQUALITY', 0)\n",
        "dev_sexism['label']=dev_sexism['label'].replace('STEREOTYPING-DOMINANCE', 1)\n",
        "dev_sexism['label']=dev_sexism['label'].replace('OBJECTIFICATION', 2)\n",
        "dev_sexism['label']=dev_sexism['label'].replace('MISOGYNY-NON-SEXUAL-VIOLENCE', 3)\n",
        "dev_sexism['label']=dev_sexism['label'].replace('SEXUAL-VIOLENCE', 4)\n",
        "dev_sexism['label'].value_counts()"
      ],
      "metadata": {
        "id": "Twyaw3SefNHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding using TF-IDF"
      ],
      "metadata": {
        "id": "yDNjBRPpanSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
        "tfidf = tfidf_vectorizer.fit_transform(df_tweets['processed_tweets'])\n",
        "\n",
        "print(tfidf.shape)\n",
        "\n",
        "X = tfidf\n",
        "y = df_tweets['label'].astype(int)\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
      ],
      "metadata": {
        "id": "bb3CZ0A5ZAvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "tfidf_vectorizer_dev = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
        "tfidf_dev = tfidf_vectorizer_dev.fit_transform(dev_tweets['processed_tweets'])\n",
        "\n",
        "print(tfidf_dev.shape)\n",
        "\n",
        "X_dev = tfidf_dev\n",
        "y_dev = dev_tweets['label'].astype(int)\n",
        "X_dev_train_tfidf, X_dev_test_tfidf, y_dev_train, y_dev_test = train_test_split(X_dev, y_dev, random_state=42, test_size=0.1)"
      ],
      "metadata": {
        "id": "ObQCEe6cfaKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning of Random Forest"
      ],
      "metadata": {
        "id": "D6bTxkjifzmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning- RandomizedSearchCV"
      ],
      "metadata": {
        "id": "0ZhPCq49iyxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "qf8XKWic0PlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search = RandomizedSearchCV(RandomForestClassifier(),\n",
        "                                   random_grid)\n",
        "random_search.fit(X_dev_train_tfidf, y_dev_train)"
      ],
      "metadata": {
        "id": "Q1P1xmuoiycG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_estimator_)"
      ],
      "metadata": {
        "id": "Yxl_3YdtsW7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used these hyperparameters in the section random forest with hyperparameter tuning below in task 1"
      ],
      "metadata": {
        "id": "7BJDLfLO0l9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Sexism Detection"
      ],
      "metadata": {
        "id": "IdQRWsdYayC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest ( By Default Hyper Parameter Tuning )"
      ],
      "metadata": {
        "id": "JVNpQaVrbHk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "rf=RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf,y_train)\n",
        "y_preds = rf.predict(X_test_tfidf)\n",
        "# acc1=accuracy_score(y_test,y_preds)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)\n",
        "# print(\"Random Forest, Accuracy Score:\",acc1)\n",
        "rfAccuracy = 100 * accuracy_score(y_test, y_preds)"
      ],
      "metadata": {
        "id": "SepHjf52ZRoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest ( With Hyperparameter tuning using Random-Search method) "
      ],
      "metadata": {
        "id": "1P5gCbkIkivg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZpin8tn-M4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf1=RandomForestClassifier(bootstrap=False, max_depth=80, max_features='auto',\n",
        "                       min_samples_leaf=4, n_estimators=1800)\n",
        "rf1.fit(X_train_tfidf,y_train)\n",
        "y_preds = rf1.predict(X_test_tfidf)\n",
        "# acc1=accuracy_score(y_test,y_preds)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)\n",
        "# print(\"Random Forest, Accuracy Score:\",acc1)"
      ],
      "metadata": {
        "id": "Ofa_kfbHkb48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feeding Dataset to Bert Model\n"
      ],
      "metadata": {
        "id": "h-j2WGhAZ_C2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = np.array(df_tweets.tweets)\n",
        "y = np.array(df_tweets['label'].astype(int))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
        "\n"
      ],
      "metadata": {
        "id": "-ytxMs-AZ-Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "jpxYEYCOm8OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(X_test))\n",
        "# print(tf.sigmoid(bert_raw_result))\n",
        "tf.keras.utils.plot_model(classifier_model)\n",
        "\n",
        "# Initializing the loss\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.binary_accuracy\n",
        "\n",
        "# Epochs Setting\n",
        "epochs = 5\n",
        "steps_per_epoch = 10\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps, optimizer_type='adamw')"
      ],
      "metadata": {
        "id": "8E2sT42BnVYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "\n",
        "# Uncomment the line below for training purpose\n",
        "# history = classifier_model.fit(X_train,y_train,epochs=epochs)\n",
        "loss, accuracy = classifier_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "LoeuI1UynYPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "uNZKKj1gnYCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparsion b/w Random Forest and Bert"
      ],
      "metadata": {
        "id": "nnOTggs0_oJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating the dataset\n",
        "data = {'RandomForest':rfAccuracy, 'Bert': accuracy * 100, }\n",
        "courses = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(courses, values, color ='maroon',\n",
        "\t\twidth = 0.4)\n",
        "\n",
        "plt.xlabel(\"Algorithm Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Sexism Classification using Random Forest and Bert\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SyMSdpm8xUsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result Generation"
      ],
      "metadata": {
        "id": "hkGdQTN50LIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sexismCategorization = []\n",
        "\n",
        "result_Task1_Both_Labels = [] \n",
        "for id, tweets in test_set.iterrows():\n",
        "  Test = tfidf_vectorizer.transform(np.array([str(tweets['processed_tweets'])]))\n",
        "  hardLabel = rf.predict(Test)\n",
        "  softLabel = rf.predict_proba(Test)\n",
        "  \n",
        "  if hardLabel[0] == 1:\n",
        "    sexismCategorization.append([tweets.id, tweets['processed_tweets']])\n",
        "\n",
        "  sub = {\n",
        "      str(tweets.id):\n",
        "      {\n",
        "        \"Hard Label\": \"Yes\" if hardLabel[0] == 1 else \"NO\",\n",
        "        \"Soft Label\": {\"Yes\":softLabel[0][1], \"No\": softLabel[0][0] }\n",
        "      }\n",
        "  }\n",
        "  result_Task1_Both_Labels.append(sub)"
      ],
      "metadata": {
        "id": "0Y2lxst9kl3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHO5CDxIKz_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('Test_set Task 1.json', 'w') as f:\n",
        "  json.dump(result_Task1_Both_Labels ,f)\n"
      ],
      "metadata": {
        "id": "v5D8UqNc0VU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing the Trained Model"
      ],
      "metadata": {
        "id": "1qUrTZN7uZap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(rf, open('Trained_Model_Task1.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "2dctDzVMSAkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Stored Model"
      ],
      "metadata": {
        "id": "6_aE34cvuUT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model = pickle.load(open('Trained_Model_Task1.pkl', 'rb'))\n",
        "y_preds = pickled_model.predict(X_test_tfidf)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)"
      ],
      "metadata": {
        "id": "4yCsIaapTTNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Sexism Categorization\n"
      ],
      "metadata": {
        "id": "9O7lrjL2_yWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-Idf Conversion"
      ],
      "metadata": {
        "id": "5g4kQFXcQAP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
        "tfidf = tfidf_vectorizer.fit_transform(df_sexism['processed_tweets'])\n",
        "\n",
        "print(tfidf.shape)\n",
        "\n",
        "X = tfidf\n",
        "y = df_sexism['label'].astype(int)\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
      ],
      "metadata": {
        "id": "Hk8sR1jcPkiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "tfidf_vectorizer_dev = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
        "tfidf_dev = tfidf_vectorizer_dev.fit_transform(dev_sexism['processed_tweets'])\n",
        "\n",
        "print(tfidf_dev.shape)\n",
        "\n",
        "X_d = tfidf_dev\n",
        "y_d = dev_sexism['label'].astype(int)\n",
        "X_dev_train_tfidf, X_dev_test_tfidf, y_dev_train, y_dev_test = train_test_split(X_d, y_d, random_state=42, test_size=0.1)"
      ],
      "metadata": {
        "id": "9TygDBAmmzRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# model = LogisticRegression().fit(X_train_tfidf,y_train)\n",
        "# y_preds = model.predict(X_test_tfidf)\n",
        "# report = classification_report( y_test, y_preds )\n",
        "\n",
        "# print(report)"
      ],
      "metadata": {
        "id": "pwzdIxr_Ps9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning of Random Forest"
      ],
      "metadata": {
        "id": "llJ1m-UVoBJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "vc7nwt9hzkVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning -RandomizedSearchCV"
      ],
      "metadata": {
        "id": "7UCAXR29oUI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_search = RandomizedSearchCV(RandomForestClassifier(),\n",
        "                                   random_grid)\n",
        "random_search.fit(X_dev_train_tfidf, y_dev_train)"
      ],
      "metadata": {
        "id": "rQdQLWckoAry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used these parameter in the section Random-Forest-with Hyperparameter tuning"
      ],
      "metadata": {
        "id": "zTOn7_lxyrFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_estimator_)"
      ],
      "metadata": {
        "id": "GqusqMaRCW98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest ( By default Hyperparameter Tuning)"
      ],
      "metadata": {
        "id": "SvFoDEjBpD0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf=RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf,y_train)\n",
        "y_preds = rf.predict(X_test_tfidf)\n",
        "# acc1=accuracy_score(y_test,y_preds)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)\n",
        "# print(\"Random Forest, Accuracy Score:\",acc1)\n",
        "\n",
        "rfAccuracy1 = 100 * accuracy_score(y_test, y_preds)"
      ],
      "metadata": {
        "id": "tBojnLM1pAOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest ( With Hyperparameter Tuning ) "
      ],
      "metadata": {
        "id": "-lnm1Ie1pL6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf1=RandomForestClassifier(bootstrap=False, max_depth=40, max_features='auto',\n",
        "                       min_samples_leaf=4, n_estimators=1600)\n",
        "rf1.fit(X_train_tfidf,y_train)\n",
        "y_preds = rf1.predict(X_test_tfidf)\n",
        "# acc1=accuracy_score(y_test,y_preds)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)\n",
        "# print(\"Random Forest, Accuracy Score:\",acc1)"
      ],
      "metadata": {
        "id": "tH52WE_OpSkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feeding Sexism Categorization to Bert"
      ],
      "metadata": {
        "id": "7q1obyYzaInK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = np.array(df_sexism.sexism)\n",
        "y = np.array(df_sexism['label'].astype(int))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "id": "reFIw5U4aR91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "iz6fCQL5bYGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier_model = build_classifier_model()\n",
        "# bert_raw_result = classifier_model(tf.constant(X_test))\n",
        "# print(tf.sigmoid(bert_raw_result))\n",
        "tf.keras.utils.plot_model(classifier_model)\n",
        "\n",
        "# Initializing the loss\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.binary_accuracy\n",
        "\n",
        "# Epochs Setting\n",
        "epochs = 5\n",
        "steps_per_epoch = 10\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps, optimizer_type='adamw')"
      ],
      "metadata": {
        "id": "yqajiYc0baaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "\n",
        "history = classifier_model.fit(X_train, y_train, epochs=2, batch_size=32)\n",
        "loss1, accuracy1 = classifier_model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "70CuI1WObjOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Loss: {loss1}')\n",
        "print(f'Accuracy: {accuracy1}')"
      ],
      "metadata": {
        "id": "5t6GGI4kbnnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZvTDobyDVvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparsion between Random Forest and Bert Task 3"
      ],
      "metadata": {
        "id": "TfL9qkvZDeMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating the dataset\n",
        "data = {'RandomForest':rfAccuracy1, 'Bert':accuracy1 * 100, }\n",
        "courses = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(courses, values, color ='maroon',\n",
        "\t\twidth = 0.4)\n",
        "\n",
        "plt.xlabel(\"Algorithm Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Sexism Categorization using Random forest and Bert\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mDQVKqE_Dm9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Results"
      ],
      "metadata": {
        "id": "sARsgR2F79r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing the Trained Model"
      ],
      "metadata": {
        "id": "Wk6ec7Q8uwi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(rf, open('Trained_Model_task3.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "z4orXo4Zu0Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the trained Model"
      ],
      "metadata": {
        "id": "F7pVu7uUu2dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_model = pickle.load(open('Trained_Model_task3.pkl', 'rb'))\n",
        "y_preds = pickled_model.predict(X_test_tfidf)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)"
      ],
      "metadata": {
        "id": "PAw2AxH_u5t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task3_Sexism_Category = list()\n",
        "\n",
        "for id, tweet in sexismCategorization:\n",
        "  Test = tfidf_vectorizer.transform(np.array([str(tweet)]))\n",
        "  hardLabel = rf.predict(Test)\n",
        "  softLabel = rf.predict_proba(Test)\n",
        "\n",
        "  sub = {\n",
        "      str(id):\n",
        "      {\n",
        "        \"Hard Label\": \"IDEOLOGICAL-INEQUALITY\" if hardLabel[0] == 0 else \"STEREOTYPING-DOMINANCE\" if hardLabel[0] == 1 else \"OBJECTIFICATION\" if hardLabel[0] == 2 else \"MISOGYNY-NON-SEXUAL-VIOLENCE\" if hardLabel[0] == 3 else \"SEXUAL-VIOLENCE\",\n",
        "        \"Soft Label\": {\"IDEOLOGICAL-INEQUALITY\":round(softLabel[0][0],3), \"STEREOTYPING-DOMINANCE\": round(softLabel[0][1],3), \"OBJECTIFICATION\": round(softLabel[0][2],3), \"MISOGYNY-NON-SEXUAL-VIOLENCE\": round(softLabel[0][3],3), \"SEXUAL-VIOLENCE\": round(softLabel[0][3],3) }\n",
        "      }\n",
        "  }\n",
        "\n",
        "  task3_Sexism_Category.append(sub)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "1pRaBxkGbWTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('Task 3.json', 'w') as f:\n",
        "  json.dump(task3_Sexism_Category ,f)"
      ],
      "metadata": {
        "id": "dBjufTK3BXJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crmH6xR5sQfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}